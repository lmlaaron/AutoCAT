{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3462b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-22 14:42:33,021\tWARNING deprecation.py:45 -- DeprecationWarning: `ray.rllib.utils.torch_ops.[...]` has been deprecated. Use `ray.rllib.utils.torch_utils.[...]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "# using ray 1.9 to run\n",
    "# python 3.9\n",
    "\n",
    "from ray.rllib.agents.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "from ray.rllib.agents.a3c.a3c_torch_policy import A3CTorchPolicy\n",
    "from ray.rllib.agents.a3c.a2c import A2CTrainer\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "import gym\n",
    "import ray.tune as tune\n",
    "from torch.nn import functional as F\n",
    "from typing import Optional, Dict\n",
    "import torch.nn as nn\n",
    "import ray\n",
    "from collections import deque\n",
    "#from ray.rllib.agents.ppo.ppo_torch_policy import ValueNetworkMixin\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.postprocessing import compute_gae_for_sample_batch, \\\n",
    "    Postprocessing\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "#from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.policy_template import build_policy_class\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import Deprecated\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_ops import apply_grad_clipping, sequence_mask\n",
    "from ray.rllib.utils.typing import TrainerConfigDict, TensorType, \\\n",
    "    PolicyID, LocalOptimizer\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "torch, nn = try_import_torch()\n",
    "from cache_guessing_game_env_impl import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3868549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_init(policy: Policy, obs_space: gym.spaces.Space, \n",
    "              action_space: gym.spaces.Space, config: TrainerConfigDict)->None:\n",
    "        #pass\n",
    "        policy.past_len = 5        \n",
    "        policy.past_models = deque(maxlen =policy.past_len)\n",
    "        policy.timestep = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24493b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a customized model that implemented __deepcopy__ properly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        layers = []\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.dim, self.dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.dim, self.dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.layers(x)\n",
    "\n",
    "class DNNEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_blocks: Optional[int] = 1) -> None:\n",
    "        super(DNNEncoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_dim, self.hidden_dim))\n",
    "        for _ in range(self.num_blocks):\n",
    "            layers.append(ResidualBlock(self.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TestModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        hidden_dim = 256 \n",
    "        super(TestModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.a_model = nn.Sequential(\n",
    "            \n",
    "            DNNEncoder(\n",
    "                input_dim=int(np.product(self.obs_space.shape)),\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=hidden_dim,\n",
    "            ),\n",
    "            nn.Linear(hidden_dim, num_outputs)\n",
    "        )\n",
    "        self.v_model = nn.Sequential(\n",
    "            DNNEncoder(\n",
    "                input_dim=int(np.product(self.obs_space.shape)),\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=hidden_dim,\n",
    "            ),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.obs_space = obs_space\n",
    "        self.action_space = action_space\n",
    "        self.num_outputs = num_outputs\n",
    "        self.model_config = model_config\n",
    "        self.name = name\n",
    "        \n",
    "        self.past_len = 5\n",
    "        self.past_models = deque(maxlen=self.past_len)\n",
    "        self.past_mean_rewards = deque(maxlen=self.past_len)\n",
    "        self._last_flat_in = None\n",
    "        \n",
    "    def __copy__(self):\n",
    "        return self\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        copied_model = TestModel(self.obs_space, self.action_space, self.num_outputs, self.model_config, self.name+'_copy')\n",
    "        copied_model.a_model.load_state_dict(self.a_model.state_dict())\n",
    "        copied_model.v_model.load_state_dict(self.v_model.state_dict())\n",
    "        return copied_model\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        #if obs[-1] > 0.99:\n",
    "        #    self.recent_model.append((copy.deepcopy(self.a_model), copy.deepcopy(self.v_model)))\n",
    "        #    if len(self.recent_model) > 5:\n",
    "        #        self.recent_model.pop()\n",
    "        obs = input_dict[\"obs_flat\"].float()\n",
    "        return self._forward(obs, input_dict, state, seq_lens)\n",
    "\n",
    "    def _forward(self, obs, input_dict, state, seq_lens):\n",
    "        self._last_flat_in = obs.reshape(obs.shape[0], -1)\n",
    "        self._output = self.a_model(self._last_flat_in)\n",
    "        return self._output, state \n",
    "\n",
    "    def value_function(self):\n",
    "        return self.v_model(self._last_flat_in).squeeze(1)\n",
    "    \n",
    "ModelCatalog.register_custom_model(\"test_model\", TestModel)\n",
    "def make_model(policy: Policy, \n",
    "               obs_space: gym.spaces.Space,\n",
    "               action_space: gym.spaces.Space, \n",
    "               model_config: TrainerConfigDict):\n",
    "    new_model = TestModel(obs_space, action_space, np.prod(action_space.shape), model_config, 'test')\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c77ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(model: ModelV2) -> ModelV2:\n",
    "    copdied_model= TorchModelV2(\n",
    "        obs_space = model.obs_space,\n",
    "        action_space = model.action_space, \n",
    "        num_outputs = model.num_outputs,\n",
    "        model_config = model.model_config,\n",
    "        name = 'copied')\n",
    "    \n",
    "    return copied_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7ee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_div_loss(policy: Policy, model: ModelV2,\n",
    "                      dist_class: ActionDistribution,\n",
    "                      train_batch: SampleBatch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    values = model.value_function()\n",
    "    valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "    dist = dist_class(logits, model)\n",
    "    log_probs = dist.logp(train_batch[SampleBatch.ACTIONS])#.reshape(-1) \n",
    "    print('log_probs')\n",
    "    print(log_probs)\n",
    "    divs = []\n",
    "    div_metric = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "    #div_metric = nn.CrossEntropyLoss()\n",
    "    #if len(policy.past_models) > 1:\n",
    "    #    assert(policy.past_models[0].state_dict() == policy.past_models[1].state_dict())\n",
    "    \n",
    "    \n",
    "    for idx, past_model in enumerate(policy.past_models):\n",
    "        #assert(False)\n",
    "        past_logits, _ = past_model.from_batch(train_batch)\n",
    "        past_values = past_model.value_function()\n",
    "        past_valid_mask = torch.ones_like(past_values, dtype=torch.bool)\n",
    "        past_dist = dist_class(past_logits, past_model)\n",
    "        #past_log_probs = past_dist.logp(train_batch[SampleBatch.ACTIONS])#.reshape(-1) \n",
    "        #div = div_metric(log_probs, past_log_probs)\n",
    "        div = dist.multi_kl(past_dist)\n",
    "        #assert(\n",
    "        \n",
    "        if idx == 0 and True:#policy.timestep % 10 == 0:\n",
    "            print('past_model.state_dict()')\n",
    "            print(past_model.state_dict())\n",
    "            print('model.state_dict()')\n",
    "            print(model.state_dict())\n",
    "            #div = past_dist.multi_kl(dist)\n",
    "            print('div')\n",
    "            print(div)\n",
    "    \n",
    "        div = div.mean(0)\n",
    "        divs.append(div)\n",
    "    print('divs')\n",
    "    print(divs)\n",
    "    div_loss = 0\n",
    "    div_loss_orig = 0\n",
    "    \n",
    "    for div in divs:\n",
    "        div_loss += div\n",
    "        div_loss_orig += div\n",
    "    div_loss = div_loss / policy.past_len\n",
    "    return div_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586fd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def custom_loss(policy: Policy, model: ModelV2,\n",
    "                      dist_class: ActionDistribution,\n",
    "                      train_batch: SampleBatch) -> TensorType:\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    values = model.value_function()\n",
    "    policy.timestep += 1\n",
    "    #if len(policy.devices) > 1:\n",
    "        # copy weights of main model (tower-0) to all other towers type\n",
    "    #if policy.timestep % 10 == 0:\n",
    "    copied_model = pickle.loads(pickle.dumps(model))\n",
    "    copied_model.load_state_dict(model.state_dict())\n",
    "        #policy.past_models.append(copied_model)\n",
    "        #policy.past_models.append(copy.deepcopy(model))\n",
    "        #policy.past_models.append(copy.copy(model))\n",
    "    \n",
    "    if policy.is_recurrent():\n",
    "        B = len(train_batch[SampleBatch.SEQ_LENS])\n",
    "        max_seq_len = logits.shape[0] // B\n",
    "        mask_orig = sequence_mask(train_batch[SampleBatch.SEQ_LENS],\n",
    "                                  max_seq_len)\n",
    "        valid_mask = torch.reshape(mask_orig, [-1])\n",
    "    else:\n",
    "        valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "    dist = dist_class(logits, model)\n",
    "    log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1)\n",
    "    \n",
    "    #print('log_probs')\n",
    "    #print(log_probs)\n",
    "    \n",
    "    pi_err = -torch.sum(\n",
    "        torch.masked_select(log_probs * train_batch[Postprocessing.ADVANTAGES],\n",
    "                            valid_mask))\n",
    "    # Compute a value function loss.\n",
    "    if policy.config[\"use_critic\"]:\n",
    "        value_err = 0.5 * torch.sum(\n",
    "            torch.pow(\n",
    "                torch.masked_select(\n",
    "                    values.reshape(-1) -\n",
    "                    train_batch[Postprocessing.VALUE_TARGETS], valid_mask),\n",
    "                2.0))\n",
    "    # Ignore the value function.\n",
    "    else:\n",
    "        value_err = 0.0\n",
    "    entropy = torch.sum(torch.masked_select(dist.entropy(), valid_mask))\n",
    "    div_loss = compute_div_loss(policy, model, dist_class, train_batch)\n",
    "    total_loss = (pi_err + value_err * policy.config[\"vf_loss_coeff\"] -\n",
    "                  entropy * policy.config[\"entropy_coeff\"] - 1000 * div_loss )\n",
    "    print('pi_err')\n",
    "    print(pi_err)\n",
    "    print('value_err')\n",
    "    print(value_err)\n",
    "    print('div_loss')\n",
    "    print(div_loss)\n",
    "    \n",
    "    # Store values for stats function in model (tower), such that for\n",
    "    # multi-GPU, we do not override them during the parallel loss phase.\n",
    "    model.tower_stats[\"entropy\"] = entropy\n",
    "    model.tower_stats[\"pi_err\"] = pi_err\n",
    "    model.tower_stats[\"value_err\"] = value_err\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ce86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomPolicy = A3CTorchPolicy.with_updates(\n",
    "    name=\"MyCustomA3CTorchPolicy\",\n",
    "    loss_fn=custom_loss,\n",
    "    #make_model= make_model,\n",
    "    before_init=custom_init)\n",
    "CustomTrainer = A2CTrainer.with_updates(\n",
    "    get_policy_class=lambda _: CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f7356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-22 14:42:35 (running for 00:00:00.12)<br>Memory usage on this node: 6.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/6.27 GiB heap, 0.0/3.14 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/ml2558/ray_results/A2C_2022-01-22_14-42-35<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_cache_guessing_game_env_fix_72a71_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m 2022-01-22 14:42:37,944\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m Load config from JSON\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m 2022-01-22 14:42:40,083\tWARNING deprecation.py:45 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m 2022-01-22 14:42:40,085\tWARNING deprecation.py:45 -- DeprecationWarning: `from_batch` has been deprecated. Use `ModelV2.__call__()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m 2022-01-22 14:42:40,085\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2022-01-22 14:42:40,099\tERROR trial_runner.py:958 -- Trial A2C_cache_guessing_game_env_fix_72a71_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/tune/trial_runner.py\", line 924, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py\", line 787, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/worker.py\", line 1715, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::A2C.__init__()\u001b[39m (pid=2534, ip=132.236.59.202)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 102, in __init__\n",
      "    Trainer.__init__(self, config, env, logger_creator,\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 661, in __init__\n",
      "    super().__init__(config, logger_creator, remote_checkpoint_dir,\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/tune/trainable.py\", line 121, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 113, in setup\n",
      "    super().setup(config)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 764, in setup\n",
      "    self._init(self.config, self.env_creator)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 136, in _init\n",
      "    self.workers = self._make_workers(\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1727, in _make_workers\n",
      "    return WorkerSet(\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 87, in __init__\n",
      "    remote_spaces = ray.get(self.remote_workers(\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2535, ip=132.236.59.202)\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 587, in __init__\n",
      "    self._build_policy_map(\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1551, in _build_policy_map\n",
      "    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n",
      "    self[policy_id] = class_(observation_space, action_space,\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy_template.py\", line 280, in __init__\n",
      "    self._initialize_loss_from_dummy_batch(\n",
      "  File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 845, in _initialize_loss_from_dummy_batch\n",
      "    self._loss(self, self.model, self.dist_class, train_batch)\n",
      "  File \"<ipython-input-6-417d1c5d7c70>\", line 11, in custom_loss\n",
      "_pickle.PicklingError: Can't pickle <class '__main__.TestModel'>: attribute lookup TestModel on __main__ failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m  {'architecture': {'word_size': 1, 'block_size': 1, 'write_back': True}, 'cache_1': {'blocks': 2, 'associativity': 2, 'hit_time': 1}, 'mem': {'hit_time': 1000}} \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m Initializing...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m Reset...\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m  victim address %d 0 \n",
      "Result for A2C_cache_guessing_game_env_fix_72a71_00000:\n",
      "  trial_id: 72a71_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-22 14:42:40 (running for 00:00:04.64)<br>Memory usage on this node: 6.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/6.27 GiB heap, 0.0/3.14 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/ml2558/ray_results/A2C_2022-01-22_14-42-35<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_cache_guessing_game_env_fix_72a71_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_cache_guessing_game_env_fix_72a71_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ml2558/ray_results/A2C_2022-01-22_14-42-35/A2C_cache_guessing_game_env_fix_72a71_00000_0_2022-01-22_14-42-35/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m 2022-01-22 14:42:40,097\tERROR worker.py:431 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::A2C.__init__()\u001b[39m (pid=2534, ip=132.236.59.202)\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 102, in __init__\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     Trainer.__init__(self, config, env, logger_creator,\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 661, in __init__\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     super().__init__(config, logger_creator, remote_checkpoint_dir,\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/tune/trainable.py\", line 121, in __init__\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 113, in setup\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     super().setup(config)\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 764, in setup\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self._init(self.config, self.env_creator)\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 136, in _init\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self.workers = self._make_workers(\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1727, in _make_workers\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     return WorkerSet(\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 87, in __init__\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     remote_spaces = ray.get(self.remote_workers(\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2535, ip=132.236.59.202)\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 587, in __init__\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1551, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self[policy_id] = class_(observation_space, action_space,\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy_template.py\", line 280, in __init__\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 845, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m     self._loss(self, self.model, self.dist_class, train_batch)\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m   File \"<ipython-input-6-417d1c5d7c70>\", line 11, in custom_loss\n",
      "\u001b[2m\u001b[36m(A2C pid=2534)\u001b[0m _pickle.PicklingError: Can't pickle <class '__main__.TestModel'>: attribute lookup TestModel on __main__ failed\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m 2022-01-22 14:42:40,090\tERROR worker.py:431 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2535, ip=132.236.59.202)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 587, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1551, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m     self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m     self[policy_id] = class_(observation_space, action_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy_template.py\", line 280, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m     self._initialize_loss_from_dummy_batch(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m   File \"/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 845, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m     self._loss(self, self.model, self.dist_class, train_batch)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m   File \"<ipython-input-6-417d1c5d7c70>\", line 11, in custom_loss\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2535)\u001b[0m _pickle.PicklingError: Can't pickle <class '__main__.TestModel'>: attribute lookup TestModel on __main__ failed\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [A2C_cache_guessing_game_env_fix_72a71_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6fb20136d679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;34m'framework'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'torch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m }\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#config={\"env\": 'Freeway-v0', \"num_gpus\":1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [A2C_cache_guessing_game_env_fix_72a71_00000])"
     ]
    }
   ],
   "source": [
    "#tune.run(CustomTrainer, config={\"env\": 'Frostbite-v0', \"num_gpus\":0})#, 'model': { 'custom_model': 'test_model' }})\n",
    "tune.register_env(\"cache_guessing_game_env_fix\", CacheGuessingGameEnv)#Fix)\n",
    "# Two ways of training\n",
    "# method 2b\n",
    "config = {\n",
    "    'env': 'cache_guessing_game_env_fix', #'cache_simulator_diversity_wrapper',\n",
    "    'env_config': {\n",
    "        'verbose': 1,\n",
    "        \"force_victim_hit\": False,\n",
    "        'flush_inst': False,\n",
    "        \"allow_victim_multi_access\": False,\n",
    "        \"attacker_addr_s\": 0,\n",
    "        \"attacker_addr_e\": 3,\n",
    "        \"victim_addr_s\": 0,\n",
    "        \"victim_addr_e\": 1,\n",
    "        \"reset_limit\": 1,\n",
    "        \"cache_configs\": {\n",
    "                # YAML config file for cache simulaton\n",
    "            \"architecture\": {\n",
    "              \"word_size\": 1, #bytes\n",
    "              \"block_size\": 1, #bytes\n",
    "              \"write_back\": True\n",
    "            },\n",
    "            \"cache_1\": {#required\n",
    "              \"blocks\": 2, \n",
    "              \"associativity\": 2,  \n",
    "              \"hit_time\": 1 #cycles\n",
    "            },\n",
    "            \"mem\": {#required\n",
    "              \"hit_time\": 1000 #cycles\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    #'gamma': 0.9, \n",
    "    'num_gpus': 1, \n",
    "    'num_workers': 1, \n",
    "    'num_envs_per_worker': 1, \n",
    "    #'entropy_coeff': 0.001, \n",
    "    #'num_sgd_iter': 5, \n",
    "    #'vf_loss_coeff': 1e-05, \n",
    "    'model': {\n",
    "        'custom_model': 'test_model',#'rnn', \n",
    "        #'max_seq_len': 20, \n",
    "        #'custom_model_config': {\n",
    "        #    'cell_size': 32\n",
    "        #   }\n",
    "    }, \n",
    "    'framework': 'torch',\n",
    "}\n",
    "tune.run(CustomTrainer, config=config)#config={\"env\": 'Freeway-v0', \"num_gpus\":1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c830a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c6261c556ed41f6b350217e2390063022b07e524465b6cb254b84519a5fbad7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
