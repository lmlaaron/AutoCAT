{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3657a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9af9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "from ray.rllib.agents.a3c.a3c_torch_policy import A3CTorchPolicy\n",
    "from ray.rllib.agents.a3c.a2c import A2CTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f93243a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ValueNetworkMixin' from 'ray.rllib.agents.ppo.ppo_torch_policy' (/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d3bc132733d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_torch_policy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValueNetworkMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiAgentEpisode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_gae_for_sample_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ValueNetworkMixin' from 'ray.rllib.agents.ppo.ppo_torch_policy' (/home/ml2558/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo.ppo_torch_policy import ValueNetworkMixin\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.postprocessing import compute_gae_for_sample_batch, \\\n",
    "    Postprocessing\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.policy_template import build_policy_class\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import Deprecated\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_ops import apply_grad_clipping, sequence_mask\n",
    "from ray.rllib.utils.typing import TrainerConfigDict, TensorType, \\\n",
    "    PolicyID, LocalOptimizer\n",
    "\n",
    "torch, nn = try_import_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7369b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_init(policy: Policy, obs_space: gym.spaces.Space, \n",
    "              action_space: gym.spaces.Space, config: TrainerConfigDict)->None:\n",
    "        policy.past_len = 5        \n",
    "        policy.past_models = deque(maxlen =policy.past_len)\n",
    "        policy.timestep = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_div_loss(policy: Policy, model: ModelV2,\n",
    "                      dist_class: ActionDistribution,\n",
    "                      train_batch: SampleBatch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    values = model.value_function()\n",
    "    valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "    dist = dist_class(logits, model)\n",
    "    log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1)\n",
    "    \n",
    "    divs = []\n",
    "    for idx, past_model in enumerate(policy.past_models):\n",
    "        logits, _ = past_model.from_batch(train_batch)\n",
    "        values = past_model.value_function()\n",
    "        valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "        dist = dist_class(logits, past_model)\n",
    "        past_log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1) \n",
    "        div = div_metric(log_probs, past_log_probs).sum(1)\n",
    "        div = div.mean(0)\n",
    "        divs.append(div)\n",
    "    \n",
    "    divs_sort_idx = np.argsort([d.data[0] for d in divs])\n",
    "    div_loss_orig = 0\n",
    "    for idx in divs_sort_idx:\n",
    "        div_loss += divs[idx]\n",
    "        div_loss_orig += divs[idx]\n",
    "    \n",
    "    div_loss = div_loss / self.past_len\n",
    "    \n",
    "    return div_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38500f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_loss(policy: Policy, model: ModelV2,\n",
    "                      dist_class: ActionDistribution,\n",
    "                      train_batch: SampleBatch) -> TensorType:\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    values = model.value_function()\n",
    "    policy.timestep += 1\n",
    "    \n",
    "    if policy.timestep % 100 == 0:\n",
    "        policy.past_models.append(copy.deepcopy(model))\n",
    "    \n",
    "    if policy.is_recurrent():\n",
    "        B = len(train_batch[SampleBatch.SEQ_LENS])\n",
    "        max_seq_len = logits.shape[0] // B\n",
    "        mask_orig = sequence_mask(train_batch[SampleBatch.SEQ_LENS],\n",
    "                                  max_seq_len)\n",
    "        valid_mask = torch.reshape(mask_orig, [-1])\n",
    "    else:\n",
    "        valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "\n",
    "    dist = dist_class(logits, model)\n",
    "    log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1)\n",
    "    pi_err = -torch.sum(\n",
    "        torch.masked_select(log_probs * train_batch[Postprocessing.ADVANTAGES],\n",
    "                            valid_mask))\n",
    "\n",
    "    # Compute a value function loss.\n",
    "    if policy.config[\"use_critic\"]:\n",
    "        value_err = 0.5 * torch.sum(\n",
    "            torch.pow(\n",
    "                torch.masked_select(\n",
    "                    values.reshape(-1) -\n",
    "                    train_batch[Postprocessing.VALUE_TARGETS], valid_mask),\n",
    "                2.0))\n",
    "    # Ignore the value function.\n",
    "    else:\n",
    "        value_err = 0.0\n",
    "\n",
    "    entropy = torch.sum(torch.masked_select(dist.entropy(), valid_mask))\n",
    "\n",
    "\n",
    "    total_loss = (pi_err + value_err * policy.config[\"vf_loss_coeff\"] -\n",
    "                  entropy * policy.config[\"entropy_coeff\"] - compute_div_loss(policy, model, dist_class, train_batch))\n",
    "\n",
    "    policy.entropy = entropy\n",
    "    policy.pi_err = pi_err\n",
    "    policy.value_err = value_err\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomPolicy = A3CTorchPolicy.with_updates(\n",
    "    name=\"MyCustomA3CTorchPolicy\",\n",
    "    loss_fn=actor_critic_loss,\n",
    "    after_init=after_init)\n",
    "CustomTrainer = A2CTrainer.with_updates(\n",
    "    default_policy=CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune.run(CustomTrainer, config={\"env\": 'Frostbite-v0', \"num_gpus\":1})\n",
    "\n",
    "#tune.run(CustomTrainer, config={\"env\": 'Frostbite-v0', \"num_gpus\":0})#, 'model': { 'custom_model': 'test_model' }})\n",
    "tune.register_env(\"cache_guessing_game_env_fix\", CacheGuessingGameEnv)#Fix)\n",
    "# Two ways of training\n",
    "# method 2b\n",
    "config = {\n",
    "    'env': 'cache_guessing_game_env_fix', #'cache_simulator_diversity_wrapper',\n",
    "    'env_config': {\n",
    "        'verbose': 1,\n",
    "        \"force_victim_hit\": False,\n",
    "        'flush_inst': False,\n",
    "        \"allow_victim_multi_access\": False,\n",
    "        \"attacker_addr_s\": 0,\n",
    "        \"attacker_addr_e\": 3,\n",
    "        \"victim_addr_s\": 0,\n",
    "        \"victim_addr_e\": 1,\n",
    "        \"reset_limit\": 1,\n",
    "        \"cache_configs\": {\n",
    "                # YAML config file for cache simulaton\n",
    "            \"architecture\": {\n",
    "              \"word_size\": 1, #bytes\n",
    "              \"block_size\": 1, #bytes\n",
    "              \"write_back\": True\n",
    "            },\n",
    "            \"cache_1\": {#required\n",
    "              \"blocks\": 2, \n",
    "              \"associativity\": 2,  \n",
    "              \"hit_time\": 1 #cycles\n",
    "            },\n",
    "            \"mem\": {#required\n",
    "              \"hit_time\": 1000 #cycles\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    #'gamma': 0.9, \n",
    "    'num_gpus': 1, \n",
    "    'num_workers': 1, \n",
    "    'num_envs_per_worker': 1, \n",
    "    #'entropy_coeff': 0.001, \n",
    "    #'num_sgd_iter': 5, \n",
    "    #'vf_loss_coeff': 1e-05, \n",
    "    'model': {\n",
    "        'custom_model': 'test_model',#'rnn', \n",
    "        #'max_seq_len': 20, \n",
    "        #'custom_model_config': {\n",
    "        #    'cell_size': 32\n",
    "        #   }\n",
    "    }, \n",
    "    'framework': 'torch',\n",
    "}\n",
    "tune.run(CustomTrainer, config=config)#config={\"env\": 'Freeway-v0', \"num_gpus\":1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cca43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
