{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3657a2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-22 00:17:53,039\tWARNING deprecation.py:45 -- DeprecationWarning: `ray.rllib.utils.torch_ops.[...]` has been deprecated. Use `ray.rllib.utils.torch_utils.[...]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "# using ray 1.9 to run\n",
    "# python 3.9\n",
    "\n",
    "from ray.rllib.agents.ppo.ppo_torch_policy import PPOTorchPolicy\n",
    "from ray.rllib.agents.a3c.a3c_torch_policy import A3CTorchPolicy\n",
    "from ray.rllib.agents.a3c.a2c import A2CTrainer\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "import gym\n",
    "import ray.tune as tune\n",
    "from torch.nn import functional as F\n",
    "from typing import Optional, Dict\n",
    "import torch.nn as nn\n",
    "import ray\n",
    "from collections import deque\n",
    "#from ray.rllib.agents.ppo.ppo_torch_policy import ValueNetworkMixin\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.rllib.evaluation.postprocessing import compute_gae_for_sample_batch, \\\n",
    "    Postprocessing\n",
    "from ray.rllib.models.action_dist import ActionDistribution\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "#from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.policy_template import build_policy_class\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.utils.annotations import Deprecated\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.torch_ops import apply_grad_clipping, sequence_mask\n",
    "from ray.rllib.utils.typing import TrainerConfigDict, TensorType, \\\n",
    "    PolicyID, LocalOptimizer\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "torch, nn = try_import_torch()\n",
    "from cache_guessing_game_env_impl import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7369b7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cache_guessing_game_env_impl import *\n",
    "\n",
    "def u_init(policy: Policy, obs_space: gym.spaces.Space, \n",
    "              action_space: gym.spaces.Space, config: TrainerConfigDict)->None:\n",
    "        policy.past_len = 5        \n",
    "        policy.past_models = deque(maxlen =policy.past_len)\n",
    "        policy.timestep = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5632007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a customized model that implemented __deepcopy__ properly\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        layers = []\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.dim, self.dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.dim, self.dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.layers(x)\n",
    "\n",
    "class DNNEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_blocks: Optional[int] = 1) -> None:\n",
    "        super(DNNEncoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.input_dim, self.hidden_dim))\n",
    "        for _ in range(self.num_blocks):\n",
    "            layers.append(ResidualBlock(self.hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim, self.output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TestModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        hidden_dim = 256 \n",
    "        super(TestModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        self.a_model = nn.Sequential(\n",
    "            \n",
    "            DNNEncoder(\n",
    "                input_dim=int(np.product(self.obs_space.shape)),\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=hidden_dim,\n",
    "            ),\n",
    "            nn.Linear(hidden_dim, num_outputs)\n",
    "        )\n",
    "        self.v_model = nn.Sequential(\n",
    "            DNNEncoder(\n",
    "                input_dim=int(np.product(self.obs_space.shape)),\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=hidden_dim,\n",
    "            ),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.obs_space = obs_space\n",
    "        self.action_space = action_space\n",
    "        self.num_outputs = num_outputs\n",
    "        self.model_config = model_config\n",
    "        self.name = name\n",
    "        \n",
    "        self.past_len = 5\n",
    "        self.past_models = deque(maxlen=self.past_len)\n",
    "        self.past_mean_rewards = deque(maxlen=self.past_len)\n",
    "        self._last_flat_in = None\n",
    "        \n",
    "    def __copy__(self):\n",
    "        return self\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        copied_model = TestModel(self.obs_space, self.action_space, self.num_outputs, self.model_config, self.name+'_copy')\n",
    "        copied_model.a_model.load_state_dict(self.a_model.state_dict())\n",
    "        copied_model.v_model.load_state_dict(self.v_model.state_dict())\n",
    "        return copied_model\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        #if obs[-1] > 0.99:\n",
    "        #    self.recent_model.append((copy.deepcopy(self.a_model), copy.deepcopy(self.v_model)))\n",
    "        #    if len(self.recent_model) > 5:\n",
    "        #        self.recent_model.pop()\n",
    "        obs = input_dict[\"obs_flat\"].float()\n",
    "        return self._forward(obs, input_dict, state, seq_lens)\n",
    "\n",
    "    def _forward(self, obs, input_dict, state, seq_lens):\n",
    "        self._last_flat_in = obs.reshape(obs.shape[0], -1)\n",
    "        self._output = self.a_model(self._last_flat_in)\n",
    "        return self._output, state \n",
    "\n",
    "    def value_function(self):\n",
    "        return self.v_model(self._last_flat_in).squeeze(1)\n",
    "    \n",
    "ModelCatalog.register_custom_model(\"test_model\", TestModel)\n",
    "def make_model(policy: Policy, \n",
    "               obs_space: gym.spaces.Space,\n",
    "               action_space: gym.spaces.Space, \n",
    "               model_config: TrainerConfigDict):\n",
    "    new_model = TestModel(obs_space, action_space, np.prod(action_space.shape), model_config, 'test')\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1686ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model(model: ModelV2) -> ModelV2:\n",
    "    copdied_model= TorchModelV2(\n",
    "        obs_space = model.obs_space,\n",
    "        action_space = model.action_space, \n",
    "        num_outputs = model.num_outputs,\n",
    "        model_config = model.model_config,\n",
    "        name = 'copied')\n",
    "    \n",
    "    return copied_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652e53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_div_loss(policy: Policy, model: ModelV2,\n",
    "                      dist_class: ActionDistribution,\n",
    "                      train_batch: SampleBatch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    values = model.value_function()\n",
    "    valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "    dist = dist_class(logits, model)\n",
    "    log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1)\n",
    "    \n",
    "    divs = []\n",
    "    for idx, past_model in enumerate(policy.past_models):\n",
    "        logits, _ = past_model.from_batch(train_batch)\n",
    "        values = past_model.value_function()\n",
    "        valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "        dist = dist_class(logits, past_model)\n",
    "        past_log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1) \n",
    "        div = div_metric(log_probs, past_log_probs).sum(1)\n",
    "        div = div.mean(0)\n",
    "        divs.append(div)\n",
    "    \n",
    "    divs_sort_idx = np.argsort([d.data[0] for d in divs])\n",
    "    div_loss_orig = 0\n",
    "    for idx in divs_sort_idx:\n",
    "        div_loss += divs[idx]\n",
    "        div_loss_orig += divs[idx]\n",
    "    \n",
    "    div_loss = div_loss / self.past_len\n",
    "    \n",
    "    return div_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38500f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def custom_loss(policy: Policy, model: ModelV2,\n",
    "                      dist_class: ActionDistribution,\n",
    "                      train_batch: SampleBatch) -> TensorType:\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    values = model.value_function()\n",
    "    policy.timestep += 1\n",
    "    #if len(policy.devices) > 1:\n",
    "        # copy weights of main model (tower-0) to all other towers type\n",
    "    #if policy.timestep % 10 == 0:\n",
    "    copied_model = pickle.loads(pickle.dumps(model))\n",
    "    copied_model.load_state_dict(model.state_dict())\n",
    "        #policy.past_models.append(copied_model)\n",
    "        #policy.past_models.append(copy.deepcopy(model))\n",
    "        #policy.past_models.append(copy.copy(model))\n",
    "    \n",
    "    if policy.is_recurrent():\n",
    "        B = len(train_batch[SampleBatch.SEQ_LENS])\n",
    "        max_seq_len = logits.shape[0] // B\n",
    "        mask_orig = sequence_mask(train_batch[SampleBatch.SEQ_LENS],\n",
    "                                  max_seq_len)\n",
    "        valid_mask = torch.reshape(mask_orig, [-1])\n",
    "    else:\n",
    "        valid_mask = torch.ones_like(values, dtype=torch.bool)\n",
    "    dist = dist_class(logits, model)\n",
    "    log_probs = dist.logp(train_batch[SampleBatch.ACTIONS]).reshape(-1)\n",
    "    \n",
    "    #print('log_probs')\n",
    "    #print(log_probs)\n",
    "    \n",
    "    pi_err = -torch.sum(\n",
    "        torch.masked_select(log_probs * train_batch[Postprocessing.ADVANTAGES],\n",
    "                            valid_mask))\n",
    "    # Compute a value function loss.\n",
    "    if policy.config[\"use_critic\"]:\n",
    "        value_err = 0.5 * torch.sum(\n",
    "            torch.pow(\n",
    "                torch.masked_select(\n",
    "                    values.reshape(-1) -\n",
    "                    train_batch[Postprocessing.VALUE_TARGETS], valid_mask),\n",
    "                2.0))\n",
    "    # Ignore the value function.\n",
    "    else:\n",
    "        value_err = 0.0\n",
    "    entropy = torch.sum(torch.masked_select(dist.entropy(), valid_mask))\n",
    "    div_loss = compute_div_loss(policy, model, dist_class, train_batch)\n",
    "    total_loss = (pi_err + value_err * policy.config[\"vf_loss_coeff\"] -\n",
    "                  entropy * policy.config[\"entropy_coeff\"] - 1000 * div_loss )\n",
    "    print('pi_err')\n",
    "    print(pi_err)\n",
    "    print('value_err')\n",
    "    print(value_err)\n",
    "    print('div_loss')\n",
    "    print(div_loss)\n",
    "    \n",
    "    # Store values for stats function in model (tower), such that for\n",
    "    # multi-GPU, we do not override them during the parallel loss phase.\n",
    "    model.tower_stats[\"entropy\"] = entropy\n",
    "    model.tower_stats[\"pi_err\"] = pi_err\n",
    "    model.tower_stats[\"value_err\"] = value_err\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578e90ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m CustomPolicy \u001b[38;5;241m=\u001b[39m A3CTorchPolicy\u001b[38;5;241m.\u001b[39mwith_updates(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyCustomA3CTorchPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mcustom_loss,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#make_model= make_model,\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     before_init\u001b[38;5;241m=\u001b[39m\u001b[43mcustom_init\u001b[49m)\n\u001b[1;32m      6\u001b[0m CustomTrainer \u001b[38;5;241m=\u001b[39m A2CTrainer\u001b[38;5;241m.\u001b[39mwith_updates(\n\u001b[1;32m      7\u001b[0m     get_policy_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m _: CustomPolicy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_init' is not defined"
     ]
    }
   ],
   "source": [
    "CustomPolicy = A3CTorchPolicy.with_updates(\n",
    "    name=\"MyCustomA3CTorchPolicy\",\n",
    "    loss_fn=custom_loss,\n",
    "    #make_model= make_model,\n",
    "    before_init=custom_init)\n",
    "CustomTrainer = A2CTrainer.with_updates(\n",
    "    get_policy_class=lambda _: CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune.run(CustomTrainer, config={\"env\": 'Frostbite-v0', \"num_gpus\":1})\n",
    "\n",
    "#tune.run(CustomTrainer, config={\"env\": 'Frostbite-v0', \"num_gpus\":0})#, 'model': { 'custom_model': 'test_model' }})\n",
    "tune.register_env(\"cache_guessing_game_env_fix\", CacheGuessingGameEnv)#Fix)\n",
    "# Two ways of training\n",
    "# method 2b\n",
    "config = {\n",
    "    'env': 'cache_guessing_game_env_fix', #'cache_simulator_diversity_wrapper',\n",
    "    'env_config': {\n",
    "        'verbose': 1,\n",
    "        \"force_victim_hit\": False,\n",
    "        'flush_inst': False,\n",
    "        \"allow_victim_multi_access\": False,\n",
    "        \"attacker_addr_s\": 0,\n",
    "        \"attacker_addr_e\": 3,\n",
    "        \"victim_addr_s\": 0,\n",
    "        \"victim_addr_e\": 1,\n",
    "        \"reset_limit\": 1,\n",
    "        \"cache_configs\": {\n",
    "                # YAML config file for cache simulaton\n",
    "            \"architecture\": {\n",
    "              \"word_size\": 1, #bytes\n",
    "              \"block_size\": 1, #bytes\n",
    "              \"write_back\": True\n",
    "            },\n",
    "            \"cache_1\": {#required\n",
    "              \"blocks\": 2, \n",
    "              \"associativity\": 2,  \n",
    "              \"hit_time\": 1 #cycles\n",
    "            },\n",
    "            \"mem\": {#required\n",
    "              \"hit_time\": 1000 #cycles\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    #'gamma': 0.9, \n",
    "    'num_gpus': 1, \n",
    "    'num_workers': 1, \n",
    "    'num_envs_per_worker': 1, \n",
    "    #'entropy_coeff': 0.001, \n",
    "    #'num_sgd_iter': 5, \n",
    "    #'vf_loss_coeff': 1e-05, \n",
    "    'model': {\n",
    "        'custom_model': 'test_model',#'rnn', \n",
    "        #'max_seq_len': 20, \n",
    "        #'custom_model_config': {\n",
    "        #    'cell_size': 32\n",
    "        #   }\n",
    "    }, \n",
    "    'framework': 'torch',\n",
    "}\n",
    "tune.run(CustomTrainer, config=config)#config={\"env\": 'Freeway-v0', \"num_gpus\":1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cca43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
